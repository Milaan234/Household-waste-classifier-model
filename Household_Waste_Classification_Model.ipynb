{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOrH2vFvQPHvEyIEi5lp+if"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdSbMGWoyyQX"
      },
      "outputs": [],
      "source": [
        "# Simple Keras CNN for household waste classifier with achieved 70-75 percent accuracy\n",
        "\n",
        "# import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for consistent results\n",
        "SEED_NUMBER = 42\n",
        "random.seed(SEED_NUMBER)\n",
        "np.random.seed(SEED_NUMBER)\n",
        "tf.random.set_seed(SEED_NUMBER)\n",
        "tf.config.experimental.enable_op_determinism()\n"
      ],
      "metadata": {
        "id": "y981ekjiy8Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Data if needed\n",
        "\"\"\"\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DIR = Path('/content/content/dataset/train')\n",
        "VAL_DIR = Path('/content/content/dataset/validation')\n",
        "directories_to_clean = [TRAIN_DIR, VAL_DIR]\n",
        "\n",
        "print(\"--- Starting Plan C (TensorFlow Check) ---\")\n",
        "deleted_files = 0\n",
        "for dir_path in directories_to_clean:\n",
        "    print(f\"\\nScanning: {dir_path}\")\n",
        "    if not dir_path.exists():\n",
        "        print(f\"  [ERROR] Directory does not exist: {dir_path}\")\n",
        "        continue\n",
        "\n",
        "    for file_path in dir_path.rglob('*'):\n",
        "        if file_path.is_dir():\n",
        "            continue\n",
        "\n",
        "        # Skip common non-image files first\n",
        "        if file_path.suffix.lower() not in ['.jpg', '.jpeg', '.png', '.bmp', '.gif']:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Read the file\n",
        "            file_data = tf.io.read_file(str(file_path))\n",
        "            # Try to decode it\n",
        "            tf.io.decode_image(file_data)\n",
        "        except Exception as e:\n",
        "            # This is the one!\n",
        "            print(f\"  [Deleting] TF-level corrupted file: {file_path.name}\")\n",
        "            deleted_files += 1\n",
        "            os.remove(file_path)\n",
        "\n",
        "print(f\"\\n--- Plan C Finished. Deleted {deleted_files} files. ---\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Gw0H3xUTMjJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Parameters ---\n",
        "\n",
        "IMAGE_SIZE = (180, 180)  # Resize all images to 180x180\n",
        "BATCH_SIZE = 16         # Process 16 images at a time\n",
        "TRAIN_DIR = '/content/content/dataset/train'\n",
        "VAL_DIR = '/content/content/dataset/validation'\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical' # 'binary' for 2 classes, 'categorical' for 3+\n",
        ")\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    VAL_DIR,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "num_classes = len(train_dataset.class_names)\n",
        "\n",
        "# to end early if loss is minimal\n",
        "early_stopper = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# This will watch the val_loss. If it stops improving for 2 epochs,\n",
        "# it will cut the learning rate in half (factor=0.5).\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "# Class weights\n",
        "##########################################################\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "# --- Get all the labels from your training data ---\n",
        "# This is a bit slow, but you only have to do it once.\n",
        "print(\"Calculating class weights... This may take a moment.\")\n",
        "train_labels = []\n",
        "for images, labels in train_dataset:\n",
        "    # labels is a \"one-hot\" vector [0, 1, 0, ...]\n",
        "    # np.argmax converts it back to a single number (e.g., 1)\n",
        "    train_labels.extend(np.argmax(labels, axis=1))\n",
        "\n",
        "# --- Calculate the weights ---\n",
        "# This function automatically calculates the correct \"punishment\"\n",
        "# values for each class.\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "# --- Convert to a dictionary for Keras ---\n",
        "# Keras needs it in the format {class_index: weight}\n",
        "# e.g., {0: 1.9, 1: 0.6, 2: 5.1, ...}\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "print(f\"Class Weights calculated:\")\n",
        "print(class_weights_dict)\n",
        "#################################################\n",
        "\n",
        "\n",
        "# --- Build the CNN Model ---\n",
        "# This is a simple, standard \"stack\" of layers\n",
        "model = keras.Sequential([\n",
        "    # Normalize pixel values from [0, 255] to [0, 1]\n",
        "    layers.Rescaling(1./255, input_shape=(*IMAGE_SIZE, 3)),\n",
        "\n",
        "    # --- Start Augmentation ---\n",
        "    # Randomly flip the image left-to-right\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    # Randomly rotate the image a little\n",
        "    layers.RandomRotation(0.1),\n",
        "    # Randomly zoom in a little\n",
        "    layers.RandomZoom(0.1),\n",
        "    # --- End Augmentation ---\n",
        "\n",
        "    # First \"convolutional\" block to find simple edges/colors\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Second block to find more complex shapes\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Third block for more complex shapes\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Flatten the 2D features into a 1D vector\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # A \"brain\" layer to make a final decision\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    layers.Dropout(0.40), # Drops 40% of connections\n",
        "\n",
        "    # The final output\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "custom_optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# --- Compile and Train the Model ---\n",
        "model.compile(\n",
        "    optimizer=custom_optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# to end early if loss is minimal\n",
        "early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=100,  # Let it look through the data 100 times\n",
        "    callbacks=[early_stopper, reduce_lr], # stop early if loss is minimal\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "# --- Save trained model ---\n",
        "model.save('household_waste_classifier.keras')\n"
      ],
      "metadata": {
        "id": "HllwPQX-zAo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the testing dataset\n",
        "loss, accuracy = model.evaluate(validation_dataset)\n",
        "\n",
        "# print accuracy\n",
        "print(f\"Final Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "JnYgHcQQzD9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# --- Get all true labels and predictions from the validation set ---\n",
        "print(\"Generating Confusion Matrix...\")\n",
        "all_true_labels = []\n",
        "all_pred_labels = []\n",
        "\n",
        "# Loop through the validation data\n",
        "for images, labels in validation_dataset:\n",
        "    predictions = model.predict(images)\n",
        "    # Convert one-hot vectors to class indices\n",
        "    true_indices = np.argmax(labels, axis=1)\n",
        "    pred_indices = np.argmax(predictions, axis=1)\n",
        "\n",
        "    all_true_labels.extend(true_indices)\n",
        "    all_pred_labels.extend(pred_indices)\n",
        "\n",
        "# --- Get the class names in the correct order ---\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "# --- Generate the matrix ---\n",
        "cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
        "\n",
        "# --- Plot the matrix ---\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='g', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E4jiJujdUn_t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}